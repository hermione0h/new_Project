{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from scipy.io import savemat \n",
    "import os\n",
    "from os import path\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(root):\n",
    "    dataset = []\n",
    "    with open(root) as f:\n",
    "        #print(root)\n",
    "        content = f.read().splitlines()\n",
    "    for line in content:\n",
    "        #path = np.load(line)\n",
    "        dataset.append(line)\n",
    "    return dataset\n",
    "class lateSpeech(data.Dataset):\n",
    "    def __init__(self,root1,root2):\n",
    "        self.data1 = make_dataset(root1)\n",
    "        #print(self.data1[0])\n",
    "        self.data2 = make_dataset(root2)\n",
    "\n",
    "        self.root1 = root1\n",
    "        self.root2 = root2\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        Xfile1 = self.data1[index]\n",
    "\n",
    "        de_file = self.data2[index]\n",
    "        X1 = np.load(Xfile1)\n",
    "        #X1 = normalize(X1)\n",
    "        de = np.load(de_file)\n",
    "        \n",
    "\n",
    "        return torch.from_numpy(X1).t().float(),torch.from_numpy(de).float() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputSize(x):\n",
    "    a = np.load(x)\n",
    "    b = a.shape\n",
    "    return b[0]\n",
    "def seqLen(x):\n",
    "    a = np.load(x)\n",
    "    b = a.shape\n",
    "    return b[1]\n",
    "wlen = 480\n",
    "nfft = 512\n",
    "overlap = 360\n",
    "test_set = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 442\n"
     ]
    }
   ],
   "source": [
    "input_file = '/data/liyuy/PROJECTS/DEREVERB3/timit_8k/reverb_train/train_wlen_'+ '%d'%wlen +'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'/reverb_rir5000_roomNum10_t600.9_loc500_8kHz.npy'\n",
    "new_file = '/data/liyuy/PROJECTS/DEREVERB3/timit_8k/reverb_valid/valid_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'/reverb_rir0500_roomNum10_t600.9_loc500_8kHz.npy'\n",
    "print(seqLen(new_file),seqLen(input_file))\n",
    "\n",
    "class param:\n",
    "    #img_size = (80, 80)\n",
    "    bs = 20\n",
    "    lr = 10e-4\n",
    "    epochs = 100\n",
    "    hsize = 513\n",
    "    hlayer = 6\n",
    "    osize = 1026\n",
    "    lstm_s = InputSize(input_file)\n",
    "    lstm_l = seqLen(input_file)\n",
    "    ts = 1\n",
    "\n",
    "mix1_train = '/data/liyuy/PROJECTS/DEREVERB3/block_conv/stft_address/train/train_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'.txt'\n",
    "\n",
    "de_train = \"/data/liyuy/PROJECTS/DEREVERB3/block_conv/target_address/train/train_target.txt\"\n",
    "\n",
    "mix1_val = '/data/liyuy/PROJECTS/DEREVERB3/block_conv/stft_address/valid/valid_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'.txt'\n",
    "\n",
    "de_val = \"/data/liyuy/PROJECTS/DEREVERB3/block_conv/target_address/valid/valid_target.txt\"\n",
    "\n",
    "mix1_test = '/data/liyuy/PROJECTS/DEREVERB3/block_conv/stft_address/test/test'+'%d'%test_set+'/test_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'.txt'\n",
    "\n",
    "de_test = '/data/liyuy/PROJECTS/DEREVERB3/block_conv/target_address/test/test'+'%d'%test_set+'_target.txt'\n",
    "\n",
    "\n",
    "train_dl = data.DataLoader(lateSpeech(mix1_train,de_train),\n",
    "                        batch_size=param.bs,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=torch.cuda.is_available())\n",
    "val_dl = data.DataLoader(lateSpeech(mix1_val,de_val),\n",
    "                    batch_size=param.bs,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=torch.cuda.is_available())\n",
    "\n",
    "test_dl = data.DataLoader(lateSpeech(mix1_test,de_test),\n",
    "                         batch_size=param.bs,\n",
    "                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_hn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_hn,self).__init__()\n",
    "        self.hsize = param.hsize\n",
    "        self.hlayer = param.hlayer\n",
    "        self.batchSize = param.bs\n",
    "        self.h0 = self.init_hidden(self.hsize,self.hlayer)\n",
    "        self.c0 = self.init_cell(self.hsize,self.hlayer)\n",
    "        self.lstm = nn.LSTM(param.lstm_s,self.hsize,self.hlayer,batch_first=True,bidirectional=True) \n",
    "        self.fc1 = nn.Linear(param.lstm_l*self.hsize,self.hsize)\n",
    "        self.fc2 = nn.Linear(self.hsize,param.osize)\n",
    "    def init_hidden(self,hidden_size,hidden_layer):\n",
    "        return Variable(torch.zeros(2*hidden_layer,self.batchSize, hidden_size).cuda())\n",
    "    \n",
    "    def init_cell(self,hidden_size,hidden_layer):\n",
    "        return Variable(torch.zeros(2*hidden_layer,self.batchSize, hidden_size).cuda())   \n",
    "        \n",
    "    def forward(self,sig1):\n",
    "        \n",
    "        hx = self.h0\n",
    "        cx = self.c0\n",
    "        \n",
    "        out,(hx,cx) = self.lstm(sig1,(hx,cx))\n",
    "        #print(out.shape)\n",
    "        a1 = out[:,:,0:513]\n",
    "        a2 = out[:,:,513:1026]\n",
    "        a = (a1 + a2) / 2\n",
    "        new_out = a.contiguous().view(-1,param.lstm_l * self.hsize)\n",
    "        \n",
    "        output1 = Func.relu(self.fc1(new_out))\n",
    "        \n",
    "        de_out = Func.leaky_relu(self.fc2(output1))\n",
    "        return de_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_hn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_hn,self).__init__()\n",
    "        self.hsize = param.hsize\n",
    "        self.hlayer = param.hlayer\n",
    "        self.batchSize = param.bs\n",
    "        self.h0 = self.init_hidden(self.hsize,self.hlayer)\n",
    "        self.c0 = self.init_cell(self.hsize,self.hlayer)\n",
    "        self.lstm = nn.LSTM(param.lstm_s,self.hsize,self.hlayer,batch_first=True,bidirectional=True) \n",
    "        self.fc1 = nn.Linear(param.lstm_l*self.hsize,self.hsize)\n",
    "        self.fc2 = nn.Linear(self.hsize,param.osize)\n",
    "    def init_hidden(self,hidden_size,hidden_layer):\n",
    "        return Variable(torch.zeros(2*hidden_layer,self.batchSize, hidden_size).cuda())\n",
    "    \n",
    "    def init_cell(self,hidden_size,hidden_layer):\n",
    "        return Variable(torch.zeros(2*hidden_layer,self.batchSize, hidden_size).cuda())   \n",
    "        \n",
    "    def forward(self,sig1):\n",
    "        \n",
    "        hx = self.h0\n",
    "        cx = self.c0\n",
    "        \n",
    "        out,(hx,cx) = self.lstm(sig1,(hx,cx))\n",
    "        \n",
    "        new_out = out.contiguous().view(-1,param.lstm_l * self.hsize)\n",
    "        \n",
    "        output1 = Func.relu(self.fc1(new_out))\n",
    "        \n",
    "        de_out = Func.leaky_relu(self.fc2(output1))\n",
    "        \n",
    "        \n",
    "        return de_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_hn().cuda()\n",
    "def weights(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                nn.init.normal_(param.data) \n",
    "model.apply(weights)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=param.lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_time(output,batch_size):\n",
    "    new_out = np.zeros((batch_size,1026))\n",
    "    for i in range (0,batch_size):\n",
    "        de_com = output[i].cpu().data.numpy()\n",
    "        de_com = np.squeeze(de_com)\n",
    "        de_real = de_com[0:513]\n",
    "        de_real = np.expand_dims(de_real,axis=1)\n",
    "      \n",
    "   \n",
    "        de_imag = de_com[513:]\n",
    "        de_imag = np.expand_dims(de_imag,axis=1)\n",
    "            \n",
    "            \n",
    "        fft_de = de_real + 1j * de_imag\n",
    "        #print(fft_de.shape)   \n",
    "        com_de = fft_de[1:512]\n",
    "       \n",
    "            #flip and take the complex conjugate part, and combine them together\n",
    "        com_de = np.conj(np.flip(com_de))\n",
    "        new_fft_de = np.concatenate((fft_de[0:513],com_de),axis=0)\n",
    "        \n",
    "        \n",
    "        de = np.real(np.fft.ifft(new_fft_de,n=fft_length,axis=0))\n",
    "        new_out[i,:] = np.squeeze(de)\n",
    "    return torch.from_numpy(new_out).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(dl, model):\n",
    "    loss = 0\n",
    "    for X1, y1 in dl:\n",
    "        X1, y1 = Variable(X1).cuda(), Variable(y1).cuda()\n",
    "        output = model(X1)\n",
    "    \n",
    "        loss1 = criterion(output,y1)\n",
    "\n",
    "        \n",
    "        loss += loss1.cpu().item() * param.bs\n",
    "    loss = loss / (len(val_dl.dataset))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d48f9d66bbfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mavgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "it = 0\n",
    "min_loss = np.inf\n",
    "bst_model_fpath = '/data/liyuy/PROJECTS/DEREVERB3/LSTM/exp5/model/bst_model_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'.pth'\n",
    "model.train(True)\n",
    "\n",
    "for epoch in range(1,param.epochs):\n",
    "    loss = 0.0\n",
    "    model.train(True)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for mag1,de_gtruth in train_dl:\n",
    "            #print(mag.shape)\n",
    "            mag1 = Variable(mag1.cuda())  # [N, 1, H, W]\n",
    "            de_gtruth = Variable(de_gtruth.cuda())\n",
    "\n",
    "            output = model(mag1)# [N, 2, H, W]\n",
    "            new_out = transfer_to_time(output,param.bs)\n",
    "            new_de = transfer_to_time(de_gtruth,param.bs)\n",
    "\n",
    "            pLoss1 = criterion(output,de_gtruth)\n",
    "            pLoss2 = criterion(new_out,new_de)\n",
    "            pLoss = pLoss1 + pLoss2\n",
    "            loss += pLoss.cpu().item() * param.bs\n",
    "            optim.zero_grad()\n",
    "            pLoss.backward()\n",
    "            optim.step()\n",
    "        avgLoss = loss/len(train_dl.dataset)\n",
    "      \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = get_loss(val_dl, model)\n",
    "    \n",
    "     \n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), bst_model_fpath)              \n",
    "        print('Epoch {:2}, Train Loss:{:>.9f}, Validation Loss:{:>.9f}'.format(epoch,avgLoss,min_loss))\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:0.000597104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "bst_model_fpath = '/data/liyuy/PROJECTS/DEREVERB3/LSTM/exp5/model/bst_model_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'bi.pth'\n",
    "\n",
    "\n",
    "outputPath = '/data/liyuy/PROJECTS/DEREVERB3/LSTM/exp5/output/output_de_all/test'+'%d'%test_set+'_wlen_'+'%d'%wlen+'_nfft_'+'%d'%nfft+'_overlap_'+'%d'%overlap+'bi_1024/'\n",
    "if path.exists(outputPath) == False:\n",
    "    os.mkdir(outputPath)\n",
    "model.load_state_dict(torch.load(bst_model_fpath))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "#new_count = 1\n",
    "index = 1\n",
    "Fs = 8e3\n",
    "loss = 0.0\n",
    "t60s = [0.3,0.6,0.9]\n",
    "t60_i = 0   \n",
    "i_loc = 1\n",
    "new_i = 1\n",
    "fft_length = 8000\n",
    "new_index = 1\n",
    "#len_pad = fft_length - 1024\n",
    "with torch.set_grad_enabled(False):\n",
    "    for mag1,de_gtruth in test_dl:\n",
    "            #print(mag.shape)\n",
    "        mag1 = Variable(mag1.cuda())  # [N, 1, H, W]\n",
    "        \n",
    "        de_gtruth = Variable(de_gtruth.cuda())\n",
    "        \n",
    "            # [N, H, W] with class indices (0, 1)\n",
    "        output1 = model(mag1)# [N, 2, H, W]\n",
    "        \n",
    "        pLoss = criterion(output1,de_gtruth)\n",
    "        \n",
    "        loss += pLoss.cpu().item() * param.bs\n",
    "        avgLoss = loss/len(test_dl.dataset)\n",
    "        \n",
    " \n",
    "        #padded_rir = np.zeros((len_pad,1))\n",
    "        if index != 1 and (index - 1) % 3 == 0:\n",
    "            new_i += 1\n",
    "            t60_i = 0\n",
    "            i_loc += 1\n",
    "        de_com = output1.cpu().data.numpy()\n",
    "        de_com = np.squeeze(de_com)\n",
    "        #print(de_com.shape)   \n",
    "        de_real = de_com[0:513]\n",
    "        de_real = np.expand_dims(de_real,axis=1)\n",
    "      \n",
    "   \n",
    "        de_imag = de_com[513:]\n",
    "        de_imag = np.expand_dims(de_imag,axis=1)\n",
    "            \n",
    "            \n",
    "        fft_de = de_real + 1j * de_imag\n",
    "        #print(fft_de.shape)   \n",
    "        com_de = fft_de[1:512]\n",
    "       \n",
    "            #flip and take the complex conjugate part, and combine them together\n",
    "        com_de = np.conj(np.flip(com_de))\n",
    "        new_fft_de = np.concatenate((fft_de[0:513],com_de),axis=0)\n",
    "        de1 = np.real(np.fft.ifft(new_fft_de,n=1024,axis=0))\n",
    "        \n",
    "        \n",
    "        de = np.real(np.fft.ifft(new_fft_de,n=fft_length,axis=0))\n",
    "        t60 = t60s[t60_i]\n",
    "        \n",
    "        outPath = outputPath +'te_de_'+ \"{0:0=4d}\".format(new_i) + '_t60' + '%.1f' % t60 + '_loc' + '%d' % (i_loc) +'recons.mat'\n",
    "        savemat(outPath,{'de':de1})\n",
    "            \n",
    "        index += 1\n",
    "        #i_loc += 1\n",
    "        t60_i += 1\n",
    "        new_index += 2\n",
    "    print('Test Loss:{:>.9f}'.format(avgLoss))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
